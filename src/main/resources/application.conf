// application.conf

kafka {
  // connection information for Kafka
  brokers = "localhost:9092"            // when running Flink in host (default, no env var needed)
  brokers = ${?MVRS_DSPA_KAFKA_BROKERS} // set only y docker-compose.yml; overrides default if defined (${?...})
}

elasticsearch {
  // connection information for ElasticSearch
  hosts = [
    {
      name = "localhost"                      // when running Flink in host (default, no env var needed)
      name = ${?MVRS_DSPA_ELASTICSEARCH_NODE} // set only by docker-compose.yml; overrides default if defined (${?...})
      port = 9201                             // when running Flink in host (default, no env var needed)
      port = ${?MVRS_DSPA_ELASTICSEARCH_PORT} // set only by docker-compose.yml; overrides default if defined (${?...})
      scheme = "http"
    }
  ]
}

data {
  // speedup factor:
  //
  // 100000: ->  5 minutes for 1 year in event time,  6 seconds for 1 week
  //  20000: -> 26 minutes for 1 year in event time, 30 seconds for 1 week
  //  10000: -> 53 minutes for 1 year in event time, 60 seconds for 1 week
  //
  // NOTE: for the low-volume stream (1K), 10000 seems to be a good value:
  // - no backpressure for any job observed
  // - stable checkpoints (size/time/alignment)
  //
  // for the high-volume stream (10K):
  // - active post statistics: no backpressure for speedup factor = 10000
  // - recommendations: - low backpressure on likes source; high backpressure on operator that broadcasts unioned events
  //                    - increasing checkpoint times at 10000, up to failure at 10 minutes (timeout)
  //                      with minimum checkpoint interval = 15 seconds
  //                    - stable checkpoints at speedup=1000.
  //                    - with checkpoint interval = 30 seconds and speedup factor 10000: checkpoints still eventually fail
  //                    - speedup factor 5000: still a bit too fast for checkpointing, times increasing
  //                    - speedup factor 2500: stable checkpoints
  //                      To be tested: checkpointing behavior with rocksdb state backend
  // - activity detection: stable checkpoints and no backpressure at speedup=10000
  speedup-factor = 10000

  // Random delays added when reading from csv files and writing to Kafka, in event time (subject to speedup)
  // Note:
  // - no *further* random delays are introduced when reading from Kafka.
  // - when reading from *multiple* partitions, events are inevitable unordered.
  random-delay = 30 minutes

  // The assumed maximum out-of-orderness (in event time) when reading from kafka. If smaller than random-delay,
  // late events will be produced.
  // Note that *detection* of late events depends on the watermark interval (jobs.auto-watermark-interval).
  // With large watermark intervals, some late events go unnoticed.
  max-out-of-orderness = 30 minutes // ${data.random-delay}

  // Watermark interval when reading from csv files. In event time (subject to speedup, with lower bound = 1 ms)
  csv-watermark-interval = 10 seconds

  // Partition count for newly created Kafka topics
  // - For controlled effect of random-delay > 0, set kafka partition count to 1.
  //   With partition count > 1 there are out-of-order events simply due to the interleaving of events from different
  //   partitions (*late* events can be avoided by using per-partition watermarks, but out-of-order events are
  //   inevitable across partitions. Regarding per-partition watermarks, this does not play well with the replay
  //   function back-pressuring on the Kafka consumer, as the watermark interval ignores phases of back pressure,
  //   resulting in a much higher effective watermark interval. Even with a configured watermark interval of 1ms and
  //   low-to-normal speedup factors, the effective watermark interval can become so large that the event-time
  //   increment between watermarks can be several days. Watermarks therefore have to be emitted *after* the replay
  //   time function, which again makes it hard to be precise with regard to late/non-late events when there are
  //   multiple partitions to read from)
  kafka-partition-count = 1

  // replica count for newly created Kafka topics
  kafka-replica-count = 1 // only one broker supported in the docker-compose file (needs fixing to allow "scale kafka=N")

  // When running jobs in the Flink container, docker-compose.yml sets MVRS_DSPA_DATA_DIR to
  // file:///usr/share/flink/data, which is mapped to a host directory that can be defined using
  // MVRS_DSPA_DATA_DIR_HOST (default: directory <projecct-directory>/docker/data)
  // so to run Flink jobs ...
  // - on HOST:      set MVRS_DSPA_DATA_DIR to file:///<local-path-to-parent-dir-of-tables-and-streams>
  // - in CONTAINER: set MVRS_DSPA_DATA_DIR_HOST to local path to parent directory of ./tables and ./streams

  // Parent directories containing test data
  // - NOTE: MVRS_DSPA_DATA_DIR must contain scheme and *absolute* path. Even on windows it must start
  //   with "/" ("/c:/..." -> file:///c:/xyz...) Example on windows: file:///c:/data/dspa/project/1k-users-sorted
  tables-directory = ${MVRS_DSPA_DATA_DIR}"/tables"
  streams-directory = ${MVRS_DSPA_DATA_DIR}"/streams"

  // test data files for events
  likes-csv-path = ${data.streams-directory}"/likes_event_stream.csv"
  posts-csv-path = ${data.streams-directory}"/post_event_stream.csv"
  comments-csv-path = ${data.streams-directory}"/comment_event_stream.csv"
}

jobs {
  remote-env {
    // NOTE submitting to a remote cluster directly from the IDE only works if the environment (paths, hostnames,
    // ports etc.) is the same both for the IDE and the cluster. This is not the case between the Flink cluster in the
    // container and the host machine. To deplay to the cluster in the container, one of the other available methods
    // (Flink CLI, REST API, web UI) has to be used.
    host = "localhost"
    port = 8082
    jars = [
      "./target/mvrs-dspa-0.9.jar" // NOTE version must match and the jar must be up-to-date ('mvn clean package' before)
    ]
  }

  // the full path (including scheme, e.g. file:///....) to the state backend
  state-backend-path = ${MVRS_DSPA_DATA_DIR}"/mvrs/flink-state-backend"

  // NOTE RocksDb state backend does not work on Windows due to https://issues.apache.org/jira/browse/FLINK-10918
  // (only observable when state gets big and wants to get flushed to disk; initially all seems ok)
  state-backend-rocksdb = true // false on Windows

  // the path to the local rocksdb storage directory
  rocksdb-path = "/tmp/mvrs-rocksdb"

  // the watermark interval when reading from Kafka
  // Note that watermarks are emitted after the replay speedup function. See discussion for data.kafka-partition-count
  auto-watermark-interval = 10 milliseconds

  // set to > 0 to enable latency tracking
  latency-tracking-interval = 0

  // checkpointing and restart strategy parameters
  checkpoint-interval = 30 seconds // processing time
  checkpoint-min-pause = 30 seconds // processing time; important to allow catching up in case of lots of buffering during alignment
  checkpoint-incrementally = true
  asynchronous-snapshots = true
  restart-attempts = 5
  delay-between-attempts = 1000 // for fixedDelayRestart strategy

  // Default parallelism for local minicluster. Applied if value is > 0
  default-local-parallelism = 4

  activity-detection {
    cluster-parameter-file-path = ${MVRS_DSPA_DATA_DIR}"/mvrs/activity-classification.txt"
    cluster-parameter-file-parse-errors-path = ${MVRS_DSPA_DATA_DIR}"/mvrs/activity-classification-errors"

    // the size and slide of the sliding window within which the frequency of interaction by person is collected
    // (which becomes part of the feature vector for clustering, along with text features of posts and comments)
    frequency-window-size = 12 hours
    frequency-window-slide = 60 minutes

    // the maximum age of a frequency result to still be relevant
    ignore-activity-older-than = 6 hours

    // default value for the number of clusters (can be modified using the control file/stream)
    default-k = 4

    // the default value for the decay factor controlling the weight influence of the previous cluster model
    // (can be modified using the control file/stream)
    default-decay = 0.2

    // the size of the sliding window within which activity feature vectors are collected, with an update of the cluster
    // model at the end of the window
    cluster-window-size = 24 hours

    // the minimum number of points required for an update of the cluster model. If the number of received points at the
    // end of the regular window is smaller, the window is extended until the point count is reached.
    minimum-cluster-element-count = 50

    // the maximum number of points that is collected until the cluster model is updated. If this number is exceeded
    // before the window ends, the cluster model is updated (early firing) and a new window is initiated.
    maximum-cluster-element.count = 20000

    // the batch size for writing classified events (classified based on the most recent available cluster model)
    // to ElasticSearch
    classified-events-elasticsearch-batch-size = 10

    // the batch size for writing cluster metadata to ElasticSearch. The metadata stream emits records only when
    // the cluster model is updated. A batch size of 1 to write any records immediately is therefore appropriate.
    cluster-metadata-elasticsearch-batch-size = 1
  }

  recommendation {
    // the size of the sliding window for gathering user activity based on which similar users are recommended
    activity-window-size = 4 hours

    // the slide duration for the sliding window
    activity-window-slide = 1 hours

    // the timeout duration for active users: a user is considered inactive if no post, comment or like event was
    // caused by this user prior to the current window end for which a recommendation is to be made
    active-users-timeout = 14 days

    // the maximum number of users to recommend
    max-recommendation-count = 5

    // the minimum value for the MinHash-approximated Jaccard similarity between a user and a user to be recommended
    min-recommendation-similarity = 0.1

    // the number of hashes to be used in a MinHash signature. The number of false negatives at a given target
    // threshold can be reduced by increasing the number of hashes, at the cost of an increased number of LSH buckets.
    minhash-num-hashes = 100

    // the target threshold for the set similarity for inclusion in the LSH selection result: a set with
    // a similarity >= threshold is included with high probability in the result. The number of false negatives at
    // that similarity can be reduced by increasing the value of num-hashes (steepening the gradient of the
    // probability curve at the threshold similarity). A low threshold value involves a higher false negative
    // probability for the same number of hashes.
    lsh-target-threshold = 0.1

    // the batch size for writing post features to ElasticSearch
    post-features-elasticsearch-batch-size = 10

    // the batch size for writing recommendations to ElasticSearch
    recommendations-elasticsearch-batch-size = 10

    // ids of persons for which intermediate/final results will be printed to console. [] to disable
    trace-person-ids = [] // [913]
  }

  active-post-statistics {
    // the size and slide of the sliding window for which activity statistcs per post are calculated and emitted
    window-size = 12 hours
    window-slide = 30 minutes

    // indicates if the author of the post should be included in the number of distinct users.
    count-post-author = false

    // the batch size for writing post statistics to ElasticSearch. A higher batch size significantly increases
    // throughput at the cost of some latency (write occurs only once the batch size is reached).
    // Note that the post statistics stream has a much higher frequency than the posts stream.
    post-statistics-elasticsearch-batch-size = 100

    // the batch size for writing post information (post content and forum title per post) to ElasticSearch.
    // this information is expected to be available in ElasticSearch once the statistics for the post are written, which
    // usually will be much later (but is not guaranteed to be so; however posts created at the very end of the sliding
    // window will not have seen much - if any - further activity by other users.
    post-info-elasticsearch-batch-size = 10

    // Partition count for the post statistics
    // When consuming the post statistics, the order per post id is important (to ensure monotonic upserts to
    // ElasticSearch). Since this is the only ordering criterion for this topic, multiple partitions and a key-based
    // partitioner is used
    kafka-partition-count = 3
  }
}
