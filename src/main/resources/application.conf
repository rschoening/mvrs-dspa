// application.conf

kafka {
  brokers = "localhost:9092"            # when running in host (default, no env var needed)
  brokers = ${?MVRS_DSPA_KAFKA_BROKERS} # when running within container (variable is set in docker-compose.yml)
}

elasticsearch {
  hosts = [
    {
      name = "localhost"
      name = ${?MVRS_DSPA_ELASTICSEARCH_NODE}
      port = 9201
      port = ${?MVRS_DSPA_ELASTICSEARCH_PORT} # overrides default if defined (${?...})
      scheme = "http"
    }
  ]
}

data {
  // 100000: ->  5 minutes for 1 year in event time,  6 seconds for 1 week
  //  20000: -> 26 minutes for 1 year in event time, 30 seconds for 1 week
  speedup-factor = 20000

  // Random delays added when reading from csv files and writing to Kafka, in event time (subject to speedup)
  // Note:
  // - no *further* random delays are introduced when reading from Kafka.
  // - when reading from *multiple* partitions, events are inevitable unordered.
  random-delay = 30 minutes

  // The assumed maximum out-of-orderness (in event time) when reading from kafka. If smaller than random-delay,
  // late events will be produced.
  // Note that *detection* of late events depends on the watermark interval (jobs.auto-watermark-interval).
  // With large intervals some late events go unnoticed.
  max-out-of-orderness = ${data.random-delay}

  // Watermark interval when reading from csv files. In event time (subject to speedup, with lower bound)
  csv-watermark-interval = 10 seconds

  // For controlled effect of random delay > 0, set kafka partition count to 1.
  // With partition count > 1 there are out-of-order events simply due to the interleaving of events from different
  // partitions (*late* events can be avoided by using per-partition watermarks, but out-of-order events are inevitable)
  kafka-partition-count = 1
  kafka-replica-count = 1 // only one broker supported in the docker-compose file (needs fixing to allow scale kafka=N)

  // NOTE: MVRS_DSPA_DATA_DIR must contain scheme and *absolute* path. Even on windows it must start with "/" ("/c:/..." -> file:///c:/xyz...)
  // example on windows: file:///c:/data/dspa/project/1k-users-sorted
  tables-directory = ${MVRS_DSPA_DATA_DIR}"/tables"
  streams-directory = ${MVRS_DSPA_DATA_DIR}"/streams"

  likes-csv-path = ${data.streams-directory}"/likes_event_stream.csv"
  posts-csv-path = ${data.streams-directory}"/post_event_stream.csv"
  comments-csv-path = ${data.streams-directory}"/comment_event_stream.csv"
}

jobs {
  state-backend-path = ${MVRS_DSPA_DATA_DIR}"/mvrs/flink-state-backend"

  // NOTE RocksDb state backend does not work on Windows due to https://issues.apache.org/jira/browse/FLINK-10918
  // (only observable when state gets big and wants to get flushed to disk; initially all seems ok)
  state-backend-rocksdb = false
  rocksdb-path = ${MVRS_DSPA_DATA_DIR}"/mvrs/rocksdb"

  auto-watermark-interval = 1 milliseconds

  latency-tracking-interval = 0

  checkpoint-interval = 30 seconds // processing time
  checkpoint-min-pause = 15 seconds // processing time
  checkpoint-incrementally = true
  asynchronous-snapshots = true
  restart-attempts = 5
  delay-between-attempts = 1000 // for fixedDelayRestart strategy

  activity-detection {
    cluster-parameter-file-path = ${MVRS_DSPA_DATA_DIR}"/mvrs/activity-classification.txt"
    cluster-parameter-file-parse-errors-path = ${MVRS_DSPA_DATA_DIR}"/mvrs/activity-classification-errors"

    frequency-window-size = 12 hours
    frequency-window-slide = 60 minutes
    default-k = 4
    default-decay = 0.2
    cluster-window-size = 24 hours
    minimum-cluster-element-count = 10
    maximum-cluster-element.count = 20000
    classified-events-elasticsearch-batch-size = 10
    cluster-metadata-elasticsearch-batch-size = 1  // infrequent
  }

  recommendation {
    activity-window-size = 4 hours
    activity-window-slide = 1 hours
    active-users-timeout = 14 days
    max-recommendation-count = 5
    min-recommendation-similarity = 0.1
    minhash-num-hashes = 100
    lsh-target-threshold = 0.1
    post-features-elasticsearch-batch-size = 10
    recommendations-elasticsearch-batch-size = 10

    // ids of persons for which intermediate/final results will be printed to console. [] to disable
    trace-person-ids = [913]
  }

  active-post-statistics {
    window-size = 12 hours
    window-slide = 30 minutes
    count-post-author = false
    post-statistics-elasticsearch-batch-size = 100
    post-info-elasticsearch-batch-size = 10
  }
}
