// application.conf

kafka {
  brokers = "localhost:9092"            // when running Flink in host (default, no env var needed)
  brokers = ${?MVRS_DSPA_KAFKA_BROKERS} // set only y docker-compose.yml; overrides default if defined (${?...})
}

elasticsearch {
  hosts = [
    {
      name = "localhost"                      // when running Flink in host (default, no env var needed)
      name = ${?MVRS_DSPA_ELASTICSEARCH_NODE} // set only by docker-compose.yml; overrides default if defined (${?...})
      port = 9201                             // when running Flink in host (default, no env var needed)
      port = ${?MVRS_DSPA_ELASTICSEARCH_PORT} // set only by docker-compose.yml; overrides default if defined (${?...})
      scheme = "http"
    }
  ]
}

data {
  // 100000: ->  5 minutes for 1 year in event time,  6 seconds for 1 week
  //  20000: -> 26 minutes for 1 year in event time, 30 seconds for 1 week
  //  10000: -> 53 minutes for 1 year in event time, 60 seconds for 1 week
  //
  // NOTE: for the low-volume stream, 10000 seems to be a good value:
  // - no backpressure for any job observed
  // - stable checkpoints (size/time/alignment)
  //
  // for the high-volume stream:
  // - active post statistics: no backpressure for speedup factor = 10000
  // - recommendations: - low backpressure on likes source; high backpressure on operator that broadcasts unioned events
  //                    - increasing checkpoint times at 10000, up to failure at 10 minutes (timeout)
  //                      with minimum checkpoint interval = 15 seconds
  //                    - stable checkpoints at speedup=1000.
  //                    - with checkpoint interval = 30 seconds and speedup factor 10000: checkpoints still eventually fail
  //                    - speedup factor 5000: still a bit too fast for checkpointing, times increasing
  //                    - speedup factor 2500:
  //                      to be tested: checkpointing behavior with rocksdb state backend
  // - activity detection: stable checkpoints and no backpressure at speedup=10000
  speedup-factor = 2500

  // Random delays added when reading from csv files and writing to Kafka, in event time (subject to speedup)
  // Note:
  // - no *further* random delays are introduced when reading from Kafka.
  // - when reading from *multiple* partitions, events are inevitable unordered.
  random-delay = 30 minutes

  // The assumed maximum out-of-orderness (in event time) when reading from kafka. If smaller than random-delay,
  // late events will be produced.
  // Note that *detection* of late events depends on the watermark interval (jobs.auto-watermark-interval).
  // With large intervals some late events go unnoticed.
  max-out-of-orderness = 30 minutes // ${data.random-delay}

  // Watermark interval when reading from csv files. In event time (subject to speedup, with lower bound = 1 ms)
  csv-watermark-interval = 10 seconds

  // For controlled effect of random delay > 0, set kafka partition count to 1.
  // With partition count > 1 there are out-of-order events simply due to the interleaving of events from different
  // partitions (*late* events can be avoided by using per-partition watermarks, but out-of-order events are inevitable)
  kafka-partition-count = 1
  kafka-replica-count = 1 // only one broker supported in the docker-compose file (needs fixing to allow "scale kafka=N")

  // When running jobs in the Flink container, docker-compose.yml sets MVRS_DSPA_DATA_DIR to
  // file:///usr/share/flink/data, which is mapped to a host directory that can be defined using
  // MVRS_DSPA_DATA_DIR_HOST (default: directory <projecct-directory>/docker/data)
  // so to run flink
  // - on HOST:      set MVRS_DSPA_DATA_DIR to file:///<local-path-to-parent-dir-of-tables-and-streams>
  // - in CONTAINER: set MVRS_DSPA_DATA_DIR_HOST to local path to parent directory of ./tables and ./streams

  // NOTE: MVRS_DSPA_DATA_DIR must contain scheme and *absolute* path. Even on windows it must start
  // with "/" ("/c:/..." -> file:///c:/xyz...) Example on windows: file:///c:/data/dspa/project/1k-users-sorted
  tables-directory = ${MVRS_DSPA_DATA_DIR}"/tables"
  streams-directory = ${MVRS_DSPA_DATA_DIR}"/streams"

  likes-csv-path = ${data.streams-directory}"/likes_event_stream.csv"
  posts-csv-path = ${data.streams-directory}"/post_event_stream.csv"
  comments-csv-path = ${data.streams-directory}"/comment_event_stream.csv"
}

jobs {
  remote-env {
    host = "localhost"
    port = 8082
    jars = [
      "./target/mvrs-dspa-0.9.jar" // NOTE make sure the version matches; requires the jar to be up-to-date (mvn clean package)
    ]
  }

  state-backend-path = ${MVRS_DSPA_DATA_DIR}"/mvrs/flink-state-backend"

  // NOTE RocksDb state backend does not work on Windows due to https://issues.apache.org/jira/browse/FLINK-10918
  // (only observable when state gets big and wants to get flushed to disk; initially all seems ok)
  state-backend-rocksdb = false
  rocksdb-path = ${MVRS_DSPA_DATA_DIR}"/mvrs/rocksdb"

  auto-watermark-interval = 1 milliseconds

  latency-tracking-interval = 0 // set to > 0 to enable latency tracking

  checkpoint-interval = 30 seconds // processing time
  checkpoint-min-pause = 30 seconds // processing time; important to allow catching up in case of lots of buffering during alignment
  checkpoint-incrementally = true
  asynchronous-snapshots = true
  restart-attempts = 5
  delay-between-attempts = 1000 // for fixedDelayRestart strategy

  // Default parallelism for local minicluster. Applied if value is > 0
  default-local-parallelism = 4

  activity-detection {
    cluster-parameter-file-path = ${MVRS_DSPA_DATA_DIR}"/mvrs/activity-classification.txt"
    cluster-parameter-file-parse-errors-path = ${MVRS_DSPA_DATA_DIR}"/mvrs/activity-classification-errors"

    frequency-window-size = 12 hours
    frequency-window-slide = 60 minutes
    default-k = 4
    default-decay = 0.2
    cluster-window-size = 24 hours
    minimum-cluster-element-count = 50
    maximum-cluster-element.count = 20000
    classified-events-elasticsearch-batch-size = 10
    cluster-metadata-elasticsearch-batch-size = 1  // infrequent, write immediately
  }

  recommendation {
    activity-window-size = 4 hours
    activity-window-slide = 1 hours
    active-users-timeout = 14 days
    max-recommendation-count = 5
    min-recommendation-similarity = 0.1
    minhash-num-hashes = 100
    lsh-target-threshold = 0.1
    post-features-elasticsearch-batch-size = 10
    recommendations-elasticsearch-batch-size = 10

    // ids of persons for which intermediate/final results will be printed to console. [] to disable
    trace-person-ids = [913]
  }

  active-post-statistics {
    window-size = 12 hours
    window-slide = 30 minutes
    count-post-author = false
    post-statistics-elasticsearch-batch-size = 100
    post-info-elasticsearch-batch-size = 10
  }
}
